{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88eddbdd-94d5-44fa-b54f-844b67966d44",
   "metadata": {},
   "source": [
    "# Neo4j Generative AI - Data Loading\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neo4j-product-examples/genai-workshop/blob/main/data-load.ipynb)\n",
    "\n",
    "This workshop will teach you to how to use Neo4j for Graph-Powered Retrieval-Augmented Generation (GraphRAG) to enhance GenAI and improve response quality for real-world applications.\n",
    "\n",
    "GenAI, despite its potential, faces challenges like hallucination and lack of domain knowledge. GraphRAG addresses these issues by combining vector search with knowledge graphs and data science techniques. This integration helps improve context, semantic understanding, and personalization, making Large Language Models (LLMs) more effective for critical applications.\n",
    "\n",
    "We walk through an example that uses real-world customer and product data from a fashion, style, and beauty retailer. We show how you can use a knowledge graph to ground an LLM, enabling it to build tailored marketing content personalized to each customer based on their interests and shared purchase histories. We use Retrieval-Augmented Generation (RAG) to accomplish this,  specifically leveraging not just vector search but also graph pattern matching and graph machine learning to provide more relevant personalized results to customers. We call this graph-powered RAG approach “GraphRAG” for short.\n",
    "\n",
    "This notebook walks through the first steps of the process, including:\n",
    "- Building the knowledge graph and generating text embeddings from scratch \n",
    "\n",
    "*If you would rather start from a database dump and skip data loading, please skip to [genai-workshop.ipynb](https://github.com/neo4j-product-examples/genai-workshop/blob/main/genai-workshop.ipynb)* \n",
    "\n",
    "\n",
    "\n",
    "Please also see the following companion notebooks to complete the workshop: \n",
    "\n",
    "- [genai-workshop.ipynb](https://github.com/neo4j-product-examples/genai-workshop/blob/main/genai-workshop.ipynb)\n",
    "    - Vector search \n",
    "    - Using graph patterns in Cypher to improve semantic search with context\n",
    "    - Further augmenting semantic search with knowledge graph inference & graph data science    \n",
    "    - Building the LLM chain and demo app for generating content \n",
    "    \n",
    "- [genai-example-app-only](https://github.com/neo4j-product-examples/genai-workshop/blob/main/genai-workshop-app-only.ipynb)\n",
    "    - Building the LLM chain and demo app for generating content \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527357f-a5e8-42c9-8966-f37846097c1d",
   "metadata": {},
   "source": [
    "### Some Logics\n",
    "1. Run the pip install below to get the necessary dependencies.  this can take a while. Then run the following cell to import relevant libraries\n",
    "2. You will need a Neo4j database environment with the [graph data science library](https://neo4j.com/docs/graph-data-science/current/installation) installed e.g. \n",
    "    - [AuraDS](https://neo4j.com/docs/aura/aurads/) \n",
    "    - [Neo4j Desktop](https://neo4j.com/docs/browser-manual/current/deployment-modes/neo4j-desktop/) \n",
    "    - Your own server environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2481cc-cf56-45d3-aa5a-236c0dbc7255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install sentence_transformers langchain langchain-openai langchain_community openai tiktoken python-dotenv gradio graphdatascience altair neo4j_tools\n",
    "%pip install \"vegafusion[embed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad71e3b-ccb9-4f5d-931f-4959862b16df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from graphdatascience import GraphDataScience\n",
    "from neo4j_tools import gds_db_load, gds_utils\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7af80e-7146-4280-a75a-7bccf16a2611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.width', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418616e-c078-448d-83c5-ab907bf08236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup Credentials and Environment Variables\n",
    "\n",
    "There are two things you need here.\n",
    "1. Credentials to a Neo4j database with Graph Data Science (AuraDS, Neo4j Desktop, or your own environment)\n",
    "2. Your own [OpenAI API key](https://platform.openai.com/docs/quickstart?context=python).  You can use [this one](https://docs.google.com/document/d/19Lqjd0MqRs088KUVnd23ZrVU9G0OAg-53U72VrFwwms/edit) if you do not have one already.\n",
    "\n",
    "To make this easy, you can write the credentials and env variables directly into the below cell.\n",
    "\n",
    "Alternatively, if you like, you can use an environments file. This is a best practice for the future, but fine to skip if you're just exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076f0ca5-c7bd-4d77-b0c8-26cc5ef7bdb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neo4j\n",
    "NEO4J_URI = 'copy_paste_your_db_uri_here' #change this\n",
    "NEO4J_PASSWORD = 'terminologies-fire-planet' #change this\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "AURA_DS = True\n",
    "\n",
    "# AI\n",
    "LLM = 'gpt-4o'\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-...' #change this\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f674b2d-da73-4d6d-be03-d259478f7cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can skip this cell if not using a ws.env file - alternative to above\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "if os.path.exists('ws.env'):\n",
    "    load_dotenv('ws.env', override=True)\n",
    "\n",
    "    # Neo4j\n",
    "    NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "    NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "    NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "    AURA_DS = eval(os.getenv('AURA_DS').title())\n",
    "\n",
    "    # AI\n",
    "    LLM = 'gpt-4o'\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9baad48-4875-4780-a168-e8eba6b95df5",
   "metadata": {},
   "source": [
    "## Knowledge Graph Building\n",
    "\n",
    "<img src=\"img/hm-banner.png\" alt=\"summary\" width=\"2000\"/>\n",
    "\n",
    "We begin by building our knowledge graph. This workshop will leverage the [H&M Personalized Fashion Recommendations Dataset](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data), a sample of real customer purchase data that includes rich information around products including names, types, descriptions, department sections, etc.\n",
    "\n",
    "Below is the graph data model we will use:\n",
    "\n",
    "<img src=\"img/data-model.png\" alt=\"summary\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37effc05-88bb-4ee4-9cbe-548a15069649",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to Neo4j\n",
    "\n",
    "We will use the [Graph Data Science Python Client](https://neo4j.com/docs/graph-data-science-client/current/) to connect to Neo4j. This client makes it convenient to display results, as we will see later.  Perhaps more importantly, it allows us to easily run [Graph Data Science](https://neo4j.com/docs/graph-data-science/current/introduction/) algorithms from Python.\n",
    "\n",
    "This client will only work if your Neo4j instance has Graph Data Science installed.  If not, you can still use the [Neo4j Python Driver](https://neo4j.com/docs/python-manual/current/) or use Langchain’s Neo4j Graph object that we will see later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cbee8fc-9699-493e-b75e-2068dd208ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Neo4j URI and credentials according to our setup\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=AURA_DS)\n",
    "\n",
    "# Necessary if you enabled Arrow on the db - this is true for AuraDS\n",
    "gds.set_database(\"neo4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2df684-9d7f-44ab-ae05-7a42b91a204e",
   "metadata": {},
   "source": [
    "Test your connection by running the below.  It should output your GDS version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a02d2d-f894-4e7b-9910-20dcc54cae5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0+73'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a831e09-3b57-43a7-b79d-e2c9845334f6",
   "metadata": {},
   "source": [
    "### Get Source Data\n",
    "This workshop will leverage the [H&M Personalized Fashion Recommendations Dataset](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data), a sample of real customer purchase data that includes rich information around products including names, types, descriptions, department sections, etc.\n",
    "\n",
    "*Bonus!*\n",
    "The data we use is a slightly sampled and preformatted version of the kaggle data. If you are interested in what we did, you can find the details [here](https://github.com/neo4j-product-examples/genai-workshop/blob/main/data-prep.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fd8856c-6132-427e-8add-f344882954b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get source data - it has been pre-formatted. If you would like to re-generate from source on Kaggle, see the data-prep.ipynb notebook\n",
    "department_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/department.csv')\n",
    "product_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/product.csv')\n",
    "article_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/article.csv')\n",
    "customer_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/customer.csv')\n",
    "transaction_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/transaction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4341e-334d-4764-aca6-8178e69c9f30",
   "metadata": {},
   "source": [
    "### Create Constraints\n",
    "\n",
    "Before loading data into Neo4j, it is usually best practice to create Key or Uniqueness constraints for nodes. These [constraints](https://neo4j.com/docs/cypher-manual/current/constraints/) act as an index with some validation on unique id properties and thus make `MATCH` statements run significantly faster. Not doing this can result in a VERY slow ingest, so this is a critical step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e37adb5-1ad3-4b71-adfa-24b5d2bf22b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create constraints - one uniqueness constraint for each node label\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_department_no IF NOT EXISTS FOR (n:Department) REQUIRE n.departmentNo IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_product_code IF NOT EXISTS FOR (n:Product) REQUIRE n.productCode IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_article_id IF NOT EXISTS FOR (n:Article) REQUIRE n.articleId IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_customer_id IF NOT EXISTS FOR (n:Customer) REQUIRE n.customerId IS UNIQUE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f331d91-f8e6-45bb-b5b2-2f10d2aafa66",
   "metadata": {},
   "source": [
    "### Loading Data with Helper Functions\n",
    "\n",
    "Since we normalized our data beforehand, we can load each node and relationship type separately in batches.\n",
    "The Node and Relationship query patterns will follow the same template for different types. The below functions simply automatically construct the queries and handle the batching.  They will print the queries they are using while loading so you can see the patterns.\n",
    "\n",
    "Cypher for Loading Nodes follows a MATCH-MERGE pattern, while Cypher for loading relationships follows a MATCH-MATCH-MERGE pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b7e4f14-2187-478c-9b03-952c8dc39f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "\n",
    "def make_map(x):\n",
    "    if type(x) == str:\n",
    "        return x, x\n",
    "    elif type(x) == tuple:\n",
    "        return x\n",
    "    else:\n",
    "        raise Exception(\"Entry must of type string or tuple\")\n",
    "\n",
    "\n",
    "def make_set_clause(prop_names: ArrayLike, element_name='n', item_name='rec'):\n",
    "    clause_list = []\n",
    "    for prop_name in prop_names:\n",
    "        clause_list.append(f'{element_name}.{prop_name} = {item_name}.{prop_name}')\n",
    "    return 'SET ' + ', '.join(clause_list)\n",
    "\n",
    "\n",
    "def make_node_merge_query(node_key_name: str, node_label: str, cols: ArrayLike):\n",
    "    template = f'''UNWIND $recs AS rec\\nMERGE(n:{node_label} {{{node_key_name}: rec.{node_key_name}}})'''\n",
    "    prop_names = [x for x in cols if x != node_key_name]\n",
    "    if len(prop_names) > 0:\n",
    "        template = template + '\\n' + make_set_clause(prop_names)\n",
    "    return template + '\\nRETURN count(n) AS nodeLoadedCount'\n",
    "\n",
    "\n",
    "def make_rel_merge_query(source_target_labels: Union[Tuple[str, str], str],\n",
    "                         source_node_key: Union[Tuple[str, str], str],\n",
    "                         target_node_key: Union[Tuple[str, str], str],\n",
    "                         rel_type: str,\n",
    "                         cols: ArrayLike,\n",
    "                         rel_key: str = None):\n",
    "    source_target_label_map = make_map(source_target_labels)\n",
    "    source_node_key_map = make_map(source_node_key)\n",
    "    target_node_key_map = make_map(target_node_key)\n",
    "\n",
    "    merge_statement = f'MERGE(s)-[r:{rel_type}]->(t)'\n",
    "    if rel_key is not None:\n",
    "        merge_statement = f'MERGE(s)-[r:{rel_type} {{{rel_key}: rec.{rel_key}}}]->(t)'\n",
    "\n",
    "    template = f'''\\tUNWIND $recs AS rec\n",
    "    MATCH(s:{source_target_label_map[0]} {{{source_node_key_map[0]}: rec.{source_node_key_map[1]}}})\n",
    "    MATCH(t:{source_target_label_map[1]} {{{target_node_key_map[0]}: rec.{target_node_key_map[1]}}})\\n\\t''' + merge_statement\n",
    "    prop_names = [x for x in cols if x not in [rel_key, source_node_key_map[1], target_node_key_map[1]]]\n",
    "    if len(prop_names) > 0:\n",
    "        template = template + '\\n\\t' + make_set_clause(prop_names, 'r')\n",
    "    return template + '\\n\\tRETURN count(r) AS relLoadedCount'\n",
    "\n",
    "\n",
    "def chunks(xs, n=10_000):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]\n",
    "\n",
    "\n",
    "def load_nodes(gds: GraphDataScience, node_df: pd.DataFrame, node_key_col: str, node_label: str, chunk_size=10_000):\n",
    "    records = node_df.to_dict('records')\n",
    "    print(f'======  loading {node_label} nodes  ======')\n",
    "    total = len(records)\n",
    "    print(f'staging {total:,} records')\n",
    "    query = make_node_merge_query(node_key_col, node_label, node_df.columns.copy())\n",
    "    print(f'\\nUsing This Cypher Query:\\n```\\n{query}\\n```\\n')\n",
    "    cumulative_count = 0\n",
    "    for recs in chunks(records, chunk_size):\n",
    "        res = gds.run_cypher(query, params={'recs': recs})\n",
    "        cumulative_count += res.iloc[0, 0]\n",
    "        print(f'Loaded {cumulative_count:,} of {total:,} nodes')\n",
    "\n",
    "\n",
    "def load_rels(gds: GraphDataScience,\n",
    "              rel_df: pd.DataFrame,\n",
    "              source_target_labels: Union[Tuple[str, str], str],\n",
    "              source_node_key: Union[Tuple[str, str], str],\n",
    "              target_node_key: Union[Tuple[str, str], str],\n",
    "              rel_type: str,\n",
    "              rel_key: str = None,\n",
    "              chunk_size=10_000):\n",
    "    records = rel_df.to_dict('records')\n",
    "    print(f'======  loading {rel_type} relationships  ======')\n",
    "    total = len(records)\n",
    "    print(f'staging {total:,} records')\n",
    "    query = make_rel_merge_query(source_target_labels, source_node_key,\n",
    "                                 target_node_key, rel_type, rel_df.columns.copy(), rel_key)\n",
    "    print(f'\\nUsing This Cypher Query:\\n```\\n{query}\\n```\\n')\n",
    "    cumulative_count = 0\n",
    "    for recs in chunks(records, chunk_size):\n",
    "        res = gds.run_cypher(query, params={'recs': recs})\n",
    "        cumulative_count += res.iloc[0, 0]\n",
    "        print(f'Loaded {cumulative_count:,} of {total:,} relationships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2f582f6-4f75-4b4f-add5-35c128c53fad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======  loading Department nodes  ======\n",
      "staging 266 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Department {departmentNo: rec.departmentNo})\n",
      "SET n.departmentName = rec.departmentName, n.sectionNo = rec.sectionNo, n.sectionName = rec.sectionName\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 266 of 266 nodes\n",
      "======  loading Article nodes  ======\n",
      "staging 13,351 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Article {articleId: rec.articleId})\n",
      "SET n.prodName = rec.prodName, n.productTypeName = rec.productTypeName, n.graphicalAppearanceNo = rec.graphicalAppearanceNo, n.graphicalAppearanceName = rec.graphicalAppearanceName, n.colourGroupCode = rec.colourGroupCode, n.colourGroupName = rec.colourGroupName\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 10,000 of 13,351 nodes\n",
      "Loaded 13,351 of 13,351 nodes\n",
      "======  loading Product nodes  ======\n",
      "staging 8,044 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Product {productCode: rec.productCode})\n",
      "SET n.prodName = rec.prodName, n.productTypeNo = rec.productTypeNo, n.productTypeName = rec.productTypeName, n.productGroupName = rec.productGroupName, n.garmentGroupNo = rec.garmentGroupNo, n.garmentGroupName = rec.garmentGroupName, n.detailDesc = rec.detailDesc\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 8,044 of 8,044 nodes\n",
      "======  loading Customer nodes  ======\n",
      "staging 1,000 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Customer {customerId: rec.customerId})\n",
      "SET n.fn = rec.fn, n.active = rec.active, n.clubMemberStatus = rec.clubMemberStatus, n.fashionNewsFrequency = rec.fashionNewsFrequency, n.age = rec.age, n.postalCode = rec.postalCode\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 1,000 of 1,000 nodes\n",
      "======  loading FROM_DEPARTMENT relationships  ======\n",
      "staging 13,351 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "\tUNWIND $recs AS rec\n",
      "    MATCH(s:Article {articleId: rec.articleId})\n",
      "    MATCH(t:Department {departmentNo: rec.departmentNo})\n",
      "\tMERGE(s)-[r:FROM_DEPARTMENT]->(t)\n",
      "\tRETURN count(r) AS relLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 10,000 of 13,351 relationships\n",
      "Loaded 13,351 of 13,351 relationships\n",
      "======  loading VARIANT_OF relationships  ======\n",
      "staging 13,351 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "\tUNWIND $recs AS rec\n",
      "    MATCH(s:Article {articleId: rec.articleId})\n",
      "    MATCH(t:Product {productCode: rec.productCode})\n",
      "\tMERGE(s)-[r:VARIANT_OF]->(t)\n",
      "\tRETURN count(r) AS relLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 10,000 of 13,351 relationships\n",
      "Loaded 13,351 of 13,351 relationships\n",
      "======  loading PURCHASED relationships  ======\n",
      "staging 23,199 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "\tUNWIND $recs AS rec\n",
      "    MATCH(s:Customer {customerId: rec.customerId})\n",
      "    MATCH(t:Article {articleId: rec.articleId})\n",
      "\tMERGE(s)-[r:PURCHASED {txId: rec.txId}]->(t)\n",
      "\tSET r.tDat = rec.tDat, r.price = rec.price, r.salesChannelId = rec.salesChannelId\n",
      "\tRETURN count(r) AS relLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 10,000 of 23,199 relationships\n",
      "Loaded 20,000 of 23,199 relationships\n",
      "Loaded 23,199 of 23,199 relationships\n",
      "CPU times: user 1.3 s, sys: 110 ms, total: 1.41 s\n",
      "Wall time: 11.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load nodes\n",
    "load_nodes(gds, department_df, 'departmentNo', 'Department')\n",
    "load_nodes(gds, article_df.drop(columns=['productCode', 'departmentNo']), 'articleId', 'Article')\n",
    "load_nodes(gds, product_df, 'productCode', 'Product')\n",
    "load_nodes(gds, customer_df, 'customerId', 'Customer')\n",
    "\n",
    "# load relationships\n",
    "load_rels(gds, article_df[['articleId', 'departmentNo']], source_target_labels=('Article', 'Department'),\n",
    "                      source_node_key='articleId', target_node_key='departmentNo',\n",
    "                      rel_type='FROM_DEPARTMENT')\n",
    "load_rels(gds, article_df[['articleId', 'productCode']], source_target_labels=('Article', 'Product'),\n",
    "                      source_node_key='articleId',target_node_key='productCode',\n",
    "                      rel_type='VARIANT_OF')\n",
    "load_rels(gds, transaction_df, source_target_labels=('Customer', 'Article'),\n",
    "                      source_node_key='customerId', target_node_key='articleId', rel_key='txId',\n",
    "                      rel_type='PURCHASED')\n",
    "\n",
    "# convert transaction dates\n",
    "gds.run_cypher('''\n",
    "MATCH (:Customer)-[r:PURCHASED]->()\n",
    "SET r.tDat = date(r.tDat)\n",
    "''')\n",
    "\n",
    "# convert NaN product descriptions\n",
    "gds.run_cypher('''\n",
    "MATCH (n:Product) WHERE valueType(n.detailDesc) <> \"STRING NOT NULL\"\n",
    "SET n.detailDesc = \"\"\n",
    "RETURN n\n",
    "''')\n",
    "\n",
    "# create combined text property. This will help simplify later with semantic search and RAG\n",
    "gds.run_cypher(\"\"\"\n",
    "    MATCH(p:Product)\n",
    "    SET p.text = 'Product-- ' +\n",
    "        'Name: ' + p.prodName + ' || ' +\n",
    "        'Type: ' + p.productTypeName + ' || ' +\n",
    "        'Group: ' + p.productGroupName + ' || ' +\n",
    "        'Garment Type: ' + p.garmentGroupName + ' || ' +\n",
    "        'Description: ' + p.detailDesc\n",
    "    RETURN count(p) AS propertySetCount\n",
    "    \"\"\")\n",
    "\n",
    "# write dummy urls to illustrate sourcing in future retrieval\n",
    "gds.run_cypher(\"\"\"\n",
    "MATCH(p:Product)\n",
    "SET p.url = 'https://representative-domain/product/' + p.productCode\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49a5c6-9732-461e-8052-ac3751ff3498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating Text Embeddings & Vector Index\n",
    "\n",
    "Now the data has been loaded, we need to generate text embeddings on our product nodes to support Vector Search\n",
    "\n",
    "Neo4j has native integrations with popular embedding APIs (OpenAI, Vertex AI, Amazon Bedrock, Azure OpenAI) making it possible to generate embeddings with a single Cypher query using `genai.vector.*` operations*.\n",
    "\n",
    "The below query embeds the Product text property with OpenAI `text-embedding-ada-002` in batches.  Specifically it\n",
    "1. Matches every Product that has a detailed description\n",
    "2. Uses the `collect` aggregation function to batch products into a set number of partitions\n",
    "3. Encodes the text property in batches using OpenAI `text-embedding-ada-002`\n",
    "4. Sets the embedding as a vector property using `db.create.setNodeVectorProperty`. This special function is used to set the properties as floats rather than double precision, which requires more space.  This becomes important as these embedding vectors tend to be long, and the size can add up quickly.\n",
    "\n",
    "*NOTE: `genai.vector.*` operations are not available in Neo4j Community Edition.  For Neo4j Community Edition you will need to generate embeddings externally and ingest them into Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "438cd7dc-b01c-48e2-b8d7-fef2bcf91aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate embeddings\n",
    "\n",
    "gds.run_cypher('''\n",
    "MATCH (n:Product) WHERE size(n.detailDesc) <> 0\n",
    "WITH collect(n) AS nodes, toInteger(rand()*$numberOfBatches) AS partition\n",
    "CALL {\n",
    "    WITH nodes\n",
    "    CALL genai.vector.encodeBatch([node IN nodes| node.text], \"OpenAI\", { token: $token})\n",
    "    YIELD index, vector\n",
    "    CALL db.create.setNodeVectorProperty(nodes[index], \"textEmbedding\", vector)\n",
    "} IN TRANSACTIONS OF 1 ROW''', params={'token':OPENAI_API_KEY, 'numberOfBatches':100})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b868b5f5-425a-4851-9351-36907909327f",
   "metadata": {},
   "source": [
    "After generating embeddings we will create a vector index for them. The Neo4j Vector Index enables efficient Approximate Nearest Neighbor (ANN) search with vectors. It uses the Hierarchical Navigable Small World (HNSW) algorithm.\n",
    "\n",
    "The below cell will create the index, then, with a separate query, await for the index to come online, meaning it is ready to be used in vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76e4ccf2-65c8-468b-90ab-0bc489eb1e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create vector index\n",
    "\n",
    "embedding_dimension = 1536 #default for OpenAI text-embedding-ada-002\n",
    "\n",
    "gds.run_cypher('''\n",
    "CREATE VECTOR INDEX product_text_embeddings IF NOT EXISTS FOR (n:Product) ON (n.textEmbedding)\n",
    "OPTIONS {indexConfig: {\n",
    " `vector.dimensions`: toInteger($dimension),\n",
    " `vector.similarity_function`: 'cosine'\n",
    "}}''', params={'dimension': embedding_dimension})\n",
    "\n",
    "#wait for index to come online\n",
    "gds.run_cypher('CALL db.awaitIndex(\"product_text_embeddings\", 300)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba071f-a3d4-4959-8236-e4d39cec0c19",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Try out a vector search with your newly made vector index, and learn how to enhance that search with graphs and graph data science! [genai-workshop.ipynb](https://github.com/neo4j-product-examples/genai-workshop/blob/main/genai-workshop.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
