{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88eddbdd-94d5-44fa-b54f-844b67966d44",
   "metadata": {},
   "source": [
    "# Neo4j Generative AI - Data Loading\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neo4j-product-examples/genai-workshop/blob/main/data-load.ipynb)\n",
    "\n",
    "This workshop will teach you how to use Neo4j for Graph-Powered Retrieval-Augmented Generation (GraphRAG) to enhance GenAI and improve response quality for real-world applications.\n",
    "\n",
    "GenAI, despite its potential, faces challenges like hallucination and lack of domain knowledge. GraphRAG addresses these issues by combining vector search with knowledge graphs and data science techniques. This integration helps improve context, semantic understanding, and personalization, making Large Language Models (LLMs) more effective for critical applications.\n",
    "\n",
    "We walk through an example that uses real-world customer and product data from a fashion, style, and beauty retailer. We show how you can use a knowledge graph to ground an LLM, enabling it to build tailored marketing content personalized to each customer based on their interests and shared purchase histories. We use Retrieval-Augmented Generation (RAG) to accomplish this,  specifically leveraging not just vector search but also graph pattern matching and graph machine learning to provide more relevant personalized results to customers. We call this graph-powered RAG approach “GraphRAG” for short.\n",
    "\n",
    "This notebook walks through the first steps of the process, including:\n",
    "- Building the knowledge graph and\n",
    "- generating text embeddings from scratch\n",
    "\n",
    "[genai-workshop.ipynb](https://github.com/neo4j-product-examples/genai-workshop/blob/main/genai-workshop.ipynb) contains the rest of the workshop including\n",
    "    - Vector search\n",
    "    - Graph patterns to improve semantic search\n",
    "    - Augmenting semantic search with graph data science\n",
    "    - Building an example LLM chain and demo app\n",
    "\n",
    "If you would rather start from a database dump and skip this data loading, you can do so using [this dump file](https://storage.googleapis.com/gds-training-materials/Version8_Jan2024/neo4j_genai_hnm.dump).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527357f-a5e8-42c9-8966-f37846097c1d",
   "metadata": {},
   "source": [
    "### Some Logistics\n",
    "1. Run the pip install below to get the necessary dependencies.  this can take a while. Then run the following cell to import relevant libraries\n",
    "2. You will need a Neo4j database environment with the [graph data science library](https://neo4j.com/docs/graph-data-science/current/installation) installed e.g. \n",
    "    - [AuraDS](https://neo4j.com/docs/aura/aurads/) \n",
    "    - [Neo4j Desktop](https://neo4j.com/docs/browser-manual/current/deployment-modes/neo4j-desktop/) \n",
    "    - Your own server environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2481cc-cf56-45d3-aa5a-236c0dbc7255",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%capture\n",
    "%pip install sentence_transformers langchain langchain-openai langchain_community openai tiktoken python-dotenv gradio graphdatascience\n",
    "%pip install \"vegafusion[embed]\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad71e3b-ccb9-4f5d-931f-4959862b16df",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from graphdatascience import GraphDataScience\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import gradio as gr"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7af80e-7146-4280-a75a-7bccf16a2611",
   "metadata": {
    "tags": []
   },
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.width', 0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0418616e-c078-448d-83c5-ab907bf08236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup Credentials and Environment Variables\n",
    "\n",
    "There are two things you need here.\n",
    "1. Credentials to a Neo4j database with Graph Data Science (AuraDS, Neo4j Desktop, or your own environment)\n",
    "2. Your own [OpenAI API key](https://platform.openai.com/docs/quickstart?context=python).  You can use [this one](https://docs.google.com/document/d/19Lqjd0MqRs088KUVnd23ZrVU9G0OAg-53U72VrFwwms/edit) if you do not have one already.\n",
    "\n",
    "To make this easy, you can write the credentials and env variables directly into the below cell.\n",
    "\n",
    "Alternatively, if you like, you can use an environment file. This is a best practice for the future, but fine to skip for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076f0ca5-c7bd-4d77-b0c8-26cc5ef7bdb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neo4j\n",
    "NEO4J_URI = 'copy_paste_your_db_uri_here' #change this\n",
    "NEO4J_PASSWORD = 'terminologies-fire-planet' #change this\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "AURA_DS = True\n",
    "\n",
    "# AI\n",
    "LLM = 'gpt-4o'\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-...' #change this\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f674b2d-da73-4d6d-be03-d259478f7cdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# You can skip this cell if not using a ws.env file - alternative to above\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "if os.path.exists('ws.env'):\n",
    "    load_dotenv('ws.env', override=True)\n",
    "\n",
    "    # Neo4j\n",
    "    NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "    NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "    NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "    AURA_DS = eval(os.getenv('AURA_DS').title())\n",
    "\n",
    "    # AI\n",
    "    LLM = 'gpt-4o'\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e9baad48-4875-4780-a168-e8eba6b95df5",
   "metadata": {},
   "source": [
    "## Knowledge Graph Building\n",
    "\n",
    "<img src=\"img/hm-banner.png\" alt=\"summary\" width=\"2000\"/>\n",
    "\n",
    "We begin by building our knowledge graph. This workshop will leverage the [H&M Personalized Fashion Recommendations Dataset](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data), a sample of real customer purchase data that includes rich information around products including names, types, descriptions, department sections, etc.\n",
    "\n",
    "Below is the graph data model we will use:\n",
    "\n",
    "<img src=\"img/data-model.png\" alt=\"summary\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37effc05-88bb-4ee4-9cbe-548a15069649",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to Neo4j\n",
    "\n",
    "We will use the [Graph Data Science Python Client](https://neo4j.com/docs/graph-data-science-client/current/) to connect to Neo4j. This client makes it convenient to display results, as we will see later.  Perhaps more importantly, it allows us to easily run [Graph Data Science](https://neo4j.com/docs/graph-data-science/current/introduction/) algorithms from Python.\n",
    "\n",
    "This client will only work if your Neo4j instance has Graph Data Science installed.  If not, you can still use the [Neo4j Python Driver](https://neo4j.com/docs/python-manual/current/) or use Langchain’s Neo4j Graph object that we will see later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cbee8fc-9699-493e-b75e-2068dd208ebb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use Neo4j URI and credentials according to our setup\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=AURA_DS)\n",
    "\n",
    "# Necessary if you enabled Arrow on the db - this is true for AuraDS\n",
    "gds.set_database(\"neo4j\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2d2df684-9d7f-44ab-ae05-7a42b91a204e",
   "metadata": {},
   "source": [
    "Test your connection by running the below.  It should output your GDS version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a02d2d-f894-4e7b-9910-20dcc54cae5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "gds.version()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8a831e09-3b57-43a7-b79d-e2c9845334f6",
   "metadata": {},
   "source": [
    "### Get Source Data\n",
    "This workshop will leverage the [H&M Personalized Fashion Recommendations Dataset](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data), a sample of real customer purchase data that includes rich information around products including names, types, descriptions, department sections, etc.\n",
    "\n",
    "*Bonus!*\n",
    "The data we use is a sampled and preformatted version of the Kaggle data. If you are interested in what we did, you can find the details [here](https://github.com/neo4j-product-examples/genai-workshop/blob/main/data-prep.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fd8856c-6132-427e-8add-f344882954b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get source data - it has been pre-formatted. If you would like to re-generate from source on Kaggle, see the data-prep.ipynb notebook\n",
    "department_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/department.csv')\n",
    "product_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/product.csv')\n",
    "article_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/article.csv')\n",
    "customer_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/customer.csv')\n",
    "transaction_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/transaction.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "18f4341e-334d-4764-aca6-8178e69c9f30",
   "metadata": {},
   "source": [
    "### Create Constraints\n",
    "\n",
    "Before loading data into Neo4j, it is usually best practice to create Key or Uniqueness constraints for nodes. These [constraints](https://neo4j.com/docs/cypher-manual/current/constraints/) act as an index with some validation on unique id properties and thus make `MATCH` statements run significantly faster. Not doing this can result in a VERY slow ingest, so this is a critical step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e37adb5-1ad3-4b71-adfa-24b5d2bf22b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# create constraints - one uniqueness constraint for each node label\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_department_no IF NOT EXISTS FOR (n:Department) REQUIRE n.departmentNo IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_product_code IF NOT EXISTS FOR (n:Product) REQUIRE n.productCode IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_article_id IF NOT EXISTS FOR (n:Article) REQUIRE n.articleId IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_customer_id IF NOT EXISTS FOR (n:Customer) REQUIRE n.customerId IS UNIQUE')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2f331d91-f8e6-45bb-b5b2-2f10d2aafa66",
   "metadata": {},
   "source": [
    "### Loading Data with Helper Functions\n",
    "\n",
    "Since we normalized our data beforehand, we can load each node and relationship type separately in batches.\n",
    "The Node and Relationship query patterns will follow the same template for different types. The below functions simply automatically construct the queries and handle the batching.  They will print the queries they are using while loading so you can see the patterns.\n",
    "\n",
    "Cypher for Loading Nodes follows a MATCH-MERGE pattern, while Cypher for loading relationships follows a MATCH-MATCH-MERGE pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b7e4f14-2187-478c-9b03-952c8dc39f6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "from typing import Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "\n",
    "def make_map(x):\n",
    "    if type(x) == str:\n",
    "        return x, x\n",
    "    elif type(x) == tuple:\n",
    "        return x\n",
    "    else:\n",
    "        raise Exception(\"Entry must of type string or tuple\")\n",
    "\n",
    "\n",
    "def make_set_clause(prop_names: ArrayLike, element_name='n', item_name='rec'):\n",
    "    clause_list = []\n",
    "    for prop_name in prop_names:\n",
    "        clause_list.append(f'{element_name}.{prop_name} = {item_name}.{prop_name}')\n",
    "    return 'SET ' + ', '.join(clause_list)\n",
    "\n",
    "\n",
    "def make_node_merge_query(node_key_name: str, node_label: str, cols: ArrayLike):\n",
    "    template = f'''UNWIND $recs AS rec\\nMERGE(n:{node_label} {{{node_key_name}: rec.{node_key_name}}})'''\n",
    "    prop_names = [x for x in cols if x != node_key_name]\n",
    "    if len(prop_names) > 0:\n",
    "        template = template + '\\n' + make_set_clause(prop_names)\n",
    "    return template + '\\nRETURN count(n) AS nodeLoadedCount'\n",
    "\n",
    "\n",
    "def make_rel_merge_query(source_target_labels: Union[Tuple[str, str], str],\n",
    "                         source_node_key: Union[Tuple[str, str], str],\n",
    "                         target_node_key: Union[Tuple[str, str], str],\n",
    "                         rel_type: str,\n",
    "                         cols: ArrayLike,\n",
    "                         rel_key: str = None):\n",
    "    source_target_label_map = make_map(source_target_labels)\n",
    "    source_node_key_map = make_map(source_node_key)\n",
    "    target_node_key_map = make_map(target_node_key)\n",
    "\n",
    "    merge_statement = f'MERGE(s)-[r:{rel_type}]->(t)'\n",
    "    if rel_key is not None:\n",
    "        merge_statement = f'MERGE(s)-[r:{rel_type} {{{rel_key}: rec.{rel_key}}}]->(t)'\n",
    "\n",
    "    template = f'''\\tUNWIND $recs AS rec\n",
    "    MATCH(s:{source_target_label_map[0]} {{{source_node_key_map[0]}: rec.{source_node_key_map[1]}}})\n",
    "    MATCH(t:{source_target_label_map[1]} {{{target_node_key_map[0]}: rec.{target_node_key_map[1]}}})\\n\\t''' + merge_statement\n",
    "    prop_names = [x for x in cols if x not in [rel_key, source_node_key_map[1], target_node_key_map[1]]]\n",
    "    if len(prop_names) > 0:\n",
    "        template = template + '\\n\\t' + make_set_clause(prop_names, 'r')\n",
    "    return template + '\\n\\tRETURN count(r) AS relLoadedCount'\n",
    "\n",
    "\n",
    "def chunks(xs, n=10_000):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]\n",
    "\n",
    "\n",
    "def load_nodes(gds: GraphDataScience, node_df: pd.DataFrame, node_key_col: str, node_label: str, chunk_size=10_000):\n",
    "    records = node_df.to_dict('records')\n",
    "    print(f'======  loading {node_label} nodes  ======')\n",
    "    total = len(records)\n",
    "    print(f'staging {total:,} records')\n",
    "    query = make_node_merge_query(node_key_col, node_label, node_df.columns.copy())\n",
    "    print(f'\\nUsing This Cypher Query:\\n```\\n{query}\\n```\\n')\n",
    "    cumulative_count = 0\n",
    "    for recs in chunks(records, chunk_size):\n",
    "        res = gds.run_cypher(query, params={'recs': recs})\n",
    "        cumulative_count += res.iloc[0, 0]\n",
    "        print(f'Loaded {cumulative_count:,} of {total:,} nodes')\n",
    "\n",
    "\n",
    "def load_rels(gds: GraphDataScience,\n",
    "              rel_df: pd.DataFrame,\n",
    "              source_target_labels: Union[Tuple[str, str], str],\n",
    "              source_node_key: Union[Tuple[str, str], str],\n",
    "              target_node_key: Union[Tuple[str, str], str],\n",
    "              rel_type: str,\n",
    "              rel_key: str = None,\n",
    "              chunk_size=10_000):\n",
    "    records = rel_df.to_dict('records')\n",
    "    print(f'======  loading {rel_type} relationships  ======')\n",
    "    total = len(records)\n",
    "    print(f'staging {total:,} records')\n",
    "    query = make_rel_merge_query(source_target_labels, source_node_key,\n",
    "                                 target_node_key, rel_type, rel_df.columns.copy(), rel_key)\n",
    "    print(f'\\nUsing This Cypher Query:\\n```\\n{query}\\n```\\n')\n",
    "    cumulative_count = 0\n",
    "    for recs in chunks(records, chunk_size):\n",
    "        res = gds.run_cypher(query, params={'recs': recs})\n",
    "        cumulative_count += res.iloc[0, 0]\n",
    "        print(f'Loaded {cumulative_count:,} of {total:,} relationships')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2f582f6-4f75-4b4f-add5-35c128c53fad",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%time\n",
    "\n",
    "# load nodes\n",
    "load_nodes(gds, department_df, 'departmentNo', 'Department')\n",
    "load_nodes(gds, article_df.drop(columns=['productCode', 'departmentNo']), 'articleId', 'Article')\n",
    "load_nodes(gds, product_df, 'productCode', 'Product')\n",
    "load_nodes(gds, customer_df, 'customerId', 'Customer')\n",
    "\n",
    "# load relationships\n",
    "load_rels(gds, article_df[['articleId', 'departmentNo']], source_target_labels=('Article', 'Department'),\n",
    "                      source_node_key='articleId', target_node_key='departmentNo',\n",
    "                      rel_type='FROM_DEPARTMENT')\n",
    "load_rels(gds, article_df[['articleId', 'productCode']], source_target_labels=('Article', 'Product'),\n",
    "                      source_node_key='articleId',target_node_key='productCode',\n",
    "                      rel_type='VARIANT_OF')\n",
    "load_rels(gds, transaction_df, source_target_labels=('Customer', 'Article'),\n",
    "                      source_node_key='customerId', target_node_key='articleId', rel_key='txId',\n",
    "                      rel_type='PURCHASED')\n",
    "\n",
    "# convert transaction dates\n",
    "gds.run_cypher('''\n",
    "MATCH (:Customer)-[r:PURCHASED]->()\n",
    "SET r.tDat = date(r.tDat)\n",
    "''')\n",
    "\n",
    "# convert NaN product descriptions\n",
    "gds.run_cypher('''\n",
    "MATCH (n:Product) WHERE valueType(n.detailDesc) <> \"STRING NOT NULL\"\n",
    "SET n.detailDesc = \"\"\n",
    "RETURN n\n",
    "''')\n",
    "\n",
    "# create combined text property. This will help simplify later with semantic search and RAG\n",
    "gds.run_cypher(\"\"\"\n",
    "    MATCH(p:Product)\n",
    "    SET p.text = 'Product-- ' +\n",
    "        'Name: ' + p.prodName + ' || ' +\n",
    "        'Type: ' + p.productTypeName + ' || ' +\n",
    "        'Group: ' + p.productGroupName + ' || ' +\n",
    "        'Garment Type: ' + p.garmentGroupName + ' || ' +\n",
    "        'Description: ' + p.detailDesc\n",
    "    RETURN count(p) AS propertySetCount\n",
    "    \"\"\")\n",
    "\n",
    "# write dummy urls to illustrate sourcing in future retrieval\n",
    "gds.run_cypher(\"\"\"\n",
    "MATCH(p:Product)\n",
    "SET p.url = 'https://representative-domain/product/' + p.productCode\n",
    "\"\"\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5d49a5c6-9732-461e-8052-ac3751ff3498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating Text Embeddings & Vector Index\n",
    "\n",
    "Now that the data has been loaded, we need to generate text embeddings on our product nodes to support Vector Search\n",
    "\n",
    "Neo4j has native integrations with popular embedding APIs (OpenAI, Vertex AI, Amazon Bedrock, Azure OpenAI) making it possible to generate embeddings with a single Cypher query using `genai.vector.*` operations*.\n",
    "\n",
    "The below query embeds the Product text property with OpenAI `text-embedding-ada-002` in batches.  Specifically it\n",
    "1. Matches every Product that has a detailed description\n",
    "2. Uses the `collect` aggregation function to batch products into a set number of partitions\n",
    "3. Encodes the text property in batches using OpenAI `text-embedding-ada-002`\n",
    "4. Sets the embedding as a vector property using `db.create.setNodeVectorProperty`. This special function is used to set the properties as floats rather than double precision, which requires more space.  This becomes important as these embedding vectors tend to be long, and the size can add up quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "438cd7dc-b01c-48e2-b8d7-fef2bcf91aa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#generate embeddings\n",
    "\n",
    "gds.run_cypher('''\n",
    "MATCH (n:Product) WHERE size(n.detailDesc) <> 0\n",
    "WITH collect(n) AS nodes, toInteger(rand()*$numberOfBatches) AS partition\n",
    "CALL {\n",
    "    WITH nodes\n",
    "    CALL genai.vector.encodeBatch([node IN nodes| node.text], \"OpenAI\", { token: $token})\n",
    "    YIELD index, vector\n",
    "    CALL db.create.setNodeVectorProperty(nodes[index], \"textEmbedding\", vector)\n",
    "} IN TRANSACTIONS OF 1 ROW''', params={'token':OPENAI_API_KEY, 'numberOfBatches':100})"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b868b5f5-425a-4851-9351-36907909327f",
   "metadata": {},
   "source": [
    "After generating embeddings we will create a vector index for them. The Neo4j Vector Index enables efficient Approximate Nearest Neighbor (ANN) search with vectors. It uses the Hierarchical Navigable Small World (HNSW) algorithm.\n",
    "\n",
    "The below cell will create the index, then, with a separate query, await for the index to come online, meaning it is ready to be used in vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76e4ccf2-65c8-468b-90ab-0bc489eb1e48",
   "metadata": {
    "tags": []
   },
   "source": [
    "#create vector index\n",
    "\n",
    "embedding_dimension = 1536 #default for OpenAI text-embedding-ada-002\n",
    "\n",
    "gds.run_cypher('''\n",
    "CREATE VECTOR INDEX product_text_embeddings IF NOT EXISTS FOR (n:Product) ON (n.textEmbedding)\n",
    "OPTIONS {indexConfig: {\n",
    " `vector.dimensions`: toInteger($dimension),\n",
    " `vector.similarity_function`: 'cosine'\n",
    "}}''', params={'dimension': embedding_dimension})\n",
    "\n",
    "#wait for index to come online\n",
    "gds.run_cypher('CALL db.awaitIndex(\"product_text_embeddings\", 300)')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a0ba071f-a3d4-4959-8236-e4d39cec0c19",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Analyze the graph, try out a vector search, and learn how to enhance search with graphs and graph data science in [genai-workshop.ipynb](https://github.com/neo4j-product-examples/genai-workshop/blob/main/genai-workshop.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
